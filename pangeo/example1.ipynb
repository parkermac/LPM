{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f68f7c-b659-499c-97f7-7d0b72f55b77",
   "metadata": {},
   "source": [
    "## Creating LiveOcean reference files using Kerchunk\n",
    "\n",
    "See https://github.com/fsspec/kerchunk for latest issues/changes and https://fsspec.github.io/kerchunk/ for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d12eab-8687-4819-832a-5c5da174aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import ujson\n",
    "from kerchunk.hdf import SingleHdf5ToZarr \n",
    "from kerchunk.combine import MultiZarrToZarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69021f77-2bd7-4d58-bc71-67b6967efa02",
   "metadata": {},
   "source": [
    "Open existing reference file and check if new dates have been added to azure data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8a6687-a233-4d7b-80db-087e14123cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference_file = 's3://esip-qhub-public/LiveOcean/LiveOcean_reference.json'  #Location where final reference will be stored\n",
    "#json_dir = 's3://esip-qhub-public/LiveOcean/individual/' #folder where individual reference files will be stored\n",
    "reference_file = '/Users/pm8/Documents/LPM/pangeo/LiveOcean_reference.json'\n",
    "json_dir = '/Users/pm8/Documents/LPM/pangeo/individual/'\n",
    "# the esip-qhub-public bucket can be read from without credentials but requires credentials to write to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e4923b-b07b-4638-a527-81e318ff324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fs = fsspec.filesystem(protocol='s3', anon = False) #Filesystem where references are saved to \n",
    "fs = fsspec.filesystem('file') #Filesystem where references are saved to \n",
    "fs_data = fsspec.filesystem('abfs', account_name='pm2') #Filesystem to open Netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3646509-d40b-426f-9a41-d566d21cd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_files = fs_data.glob('cas6-v0-u0kb/*/*.nc') #get all available netcdf files\n",
    "nc_files = [f for f in nc_files if not f.split('-')[-1].split('.')[0] == '0001'] #exclude 0001 files\n",
    "nc_files = sorted(['abfs://'+f for f in nc_files])  #prepend azure protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4421b684-b812-45ec-b2d7-1c1242d7e1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfs://cas6-v0-u0kb/f2022-07-18/ocean-his-0002.nc\n",
      "abfs://cas6-v0-u0kb/f2022-07-18/ocean-his-0003.nc\n",
      "abfs://cas6-v0-u0kb/f2022-07-18/ocean-his-0004.nc\n",
      "abfs://cas6-v0-u0kb/f2022-07-18/ocean-his-0005.nc\n",
      "abfs://cas6-v0-u0kb/f2022-07-18/ocean-his-0006.nc\n",
      "\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0021.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0022.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0023.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0024.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0025.nc\n"
     ]
    }
   ],
   "source": [
    "for nc_fn in nc_files[:5]:\n",
    "    print(nc_fn)\n",
    "print('')    \n",
    "for nc_fn in nc_files[-5:]:\n",
    "    print(nc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee2ca0cd-5dc2-4dbe-b67e-aeaa4237dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below is my attempt at updating the references daily by running this script as a cronjob. Creating the references locally \n",
    "# before the netcdf files are uploaded to azure would probably be a neater workflow\n",
    "\n",
    "from datetime import datetime, timezone, timedelta\n",
    "last_update = datetime.now(timezone.utc) - timedelta(days = 1)\n",
    "\n",
    "updated_files = []\n",
    "for f in reversed(nc_files):\n",
    "    if fs_data.info(f)['last_modified'] > last_update:\n",
    "        updated_files.append(f)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "updated_files.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d41aa1-9984-4695-8980-664efb958d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfs://cas6-v0-u0kb/f2022-08-01/ocean-his-0002.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-01/ocean-his-0003.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-01/ocean-his-0004.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-01/ocean-his-0005.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-01/ocean-his-0006.nc\n",
      "\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0021.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0022.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0023.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0024.nc\n",
      "abfs://cas6-v0-u0kb/f2022-08-03/ocean-his-0025.nc\n"
     ]
    }
   ],
   "source": [
    "for up_fn in updated_files[:5]:\n",
    "    print(up_fn)\n",
    "print('')\n",
    "for up_fn in updated_files[-5:]:\n",
    "    print(up_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45120a7d-3df0-4788-b6da-535faa3a7e24",
   "metadata": {},
   "source": [
    "Generate individual reference files for this new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10e858b-2af7-4a4c-8a53-3c50ada7b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = dict(anon=False, skip_instance_cache=True)  #arguments to fs_data.open\n",
    "\n",
    "#create unique name for each individual reference json\n",
    "def file_name(file): \n",
    "        p = file.split('/')\n",
    "        fname = p[-1]\n",
    "        dname = p[-2]\n",
    "        return f'{json_dir}{dname}_{fname}.json'\n",
    "    \n",
    "\n",
    "#generate the individual reference jsons. \n",
    "def gen_json(nc_file_url):\n",
    "    with fs_data.open(nc_file_url, **so) as infile:\n",
    "        # h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        h5chunks = SingleHdf5ToZarr(infile, nc_file_url, inline_threshold=300)\n",
    "        # inline threshold adjusts the Size below which binary blocks are included directly in the output\n",
    "        # a higher inline threshold can result in a larger json file but faster loading time\n",
    "        outf = file_name(nc_file_url)\n",
    "        with fs.open(outf, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode()); #write the individual references as jsons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5211c-a5b0-4eee-a243-f8802e0193e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(SingleHdf5ToZarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5710cce-a6c3-4eef-8c9c-dfcf64da7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this spins up a cluster on qhub, if running locally rather use the commented out cell below this one\n",
    "# import os\n",
    "# import sys\n",
    "# sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "# import ebdpy as ebd\n",
    "\n",
    "# ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "# profile = 'esip-qhub'\n",
    "# region = 'us-west-2'\n",
    "# endpoint = f's3.{region}.amazonaws.com'\n",
    "# ebd.set_credentials(profile=profile, region=region, endpoint=endpoint)\n",
    "# worker_max = 50\n",
    "# client,cluster = ebd.start_dask_cluster(profile=profile,worker_max=worker_max, \n",
    "#                                       region=region, use_existing_cluster=True,\n",
    "#                                       adaptive_scaling=True, wait_for_cluster=False, \n",
    "#                                       worker_profile='Medium Worker', \n",
    "#                                       propagate_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9301582-9916-4796-882b-fe0331f48e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()  # Launches a scheduler and workers locally\n",
    "client = Client(cluster)  # Connect to distributed cluster and override default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ecb6cc-f061-4bb2-ab37-7f62d170ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "\n",
    "# client = Client(\"tcp://127.0.0.1:59236\")\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ec12ec-6f1b-4c9f-82a1-c00a2c4adbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use dask bag to queue up the parallel computation. Similarly dask delayed could be use.\n",
    "import dask.bag\n",
    "\n",
    "b = dask.bag.from_sequence(updated_files[-3:])\n",
    "b1 = b.map(gen_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eda688-a1ab-4c83-b2d9-f902a2d595ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the gen_json computations\n",
    "b1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a86a8f-6752-40e7-bac5-bfaa1b24c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the cluster\n",
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f551f6-c157-497c-b126-a89c316ef612",
   "metadata": {},
   "source": [
    "Now we take these individual reference files and consolidate them into a single reference file. At present MultiZarrToZarr will only combine from the individual json reference files and can not append to the existing combined reference json.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0012e-cbf8-45f0-81f3-8ac1d5c4df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = fs.glob(f'{json_dir}*.json') #open all the available individual reference jsons. \n",
    "# json_list = sorted(['s3://'+f for f in json_list]) #prepend the AWS S3 protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5dbd1-c26a-4f6f-a393-25ce6f815c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topts = dict(anon=True, skip_instance_cache=True) # target options for opening the reference jsons\n",
    "ropts = dict(account_name='pm2', skip_instance_cache=True) # remote options for opening the netcdf files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadd2f8-d200-4481-8926-149f4efaf8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the combined json\n",
    "mzz = MultiZarrToZarr(json_list, \n",
    "    remote_protocol = 'abfs',\n",
    "    remote_options = ropts,\n",
    "    target_options = topts,\n",
    "    concat_dims = ['ocean_time'],\n",
    "    identical_dims=['lat_psi','lat_rho','lat_u','lat_v',\n",
    "                    'lon_psi','lon_rho','lon_u','lon_v']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14160e0-4114-4e2e-a44e-ac179ab35a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#compute the combined json and write to storage. At present this can not be run in parallel. \n",
    "with fs.open(reference_file, 'wb') as f:\n",
    "    f.write(ujson.dumps(mzz.translate()).encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0a36d-cb05-4e38-ba8d-0ab24760b8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
